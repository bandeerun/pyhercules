{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46105e7-ddb1-4e3a-90f1-26ee752e1842",
   "metadata": {},
   "source": [
    "**1. SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4fcf90-0e1b-47fb-b2b9-26fd399f4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 1.1. Install Dependencies\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "!pip install numpy Pillow torch sentence-transformers transformers accelerate google-generativeai requests huggingface_hub\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1.2. Import Libraries\n",
    "# ------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Any, Optional\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, AutoProcessor, Gemma3ForConditionalGeneration\n",
    "import google.generativeai as genai\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from huggingface_hub import login as hf_login\n",
    "import pandas as pd\n",
    "\n",
    "print(\"All base libraries imported.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1.3. Hercules Path Setup\n",
    "# ------------------------------------------------------------------------------\n",
    "try:\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add repo root to path\n",
    "    from pyhercules import Hercules\n",
    "    print(\"Hercules imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing Hercules: {e}\")\n",
    "    # sys.exit(1)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1.4. Google API Key Configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "# Load environment variables from a .env file if it exists\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    if load_dotenv():\n",
    "        print(\"Loaded environment variables from .env file.\")\n",
    "    else:\n",
    "        print(\"No .env file found, relying on system environment variables.\")\n",
    "except ImportError:\n",
    "    print(\"dotenv library not found. pip install python-dotenv\")\n",
    "print(\"Configuring Google API Key...\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# --- Option 1: Set Environment Variable (Recommended) ---\n",
    "# If GOOGLE_API_KEY is not set as an environment variable,\n",
    "# you can set it here directly. Replace \"YOUR_API_KEY_HERE\" with your actual key.\n",
    "# However, be cautious about committing keys directly into notebooks.\n",
    "# if not GOOGLE_API_KEY:\n",
    "#     GOOGLE_API_KEY = \"YOUR_API_KEY_HERE\" \n",
    "#     # os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY # Optionally set it for the current session\n",
    "\n",
    "if not GOOGLE_API_KEY or \"YOUR_API_KEY_HERE\" in GOOGLE_API_KEY:\n",
    "    print(\"WARNING: GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
    "    print(\"Google Cloud dependent examples will fail.\")\n",
    "    print(\"Please set the GOOGLE_API_KEY environment variable or update the cell above.\")\n",
    "    # To stop execution if key is absolutely mandatory for any part:\n",
    "    # sys.exit(1)\n",
    "else:\n",
    "    try:\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        print(\"Google GenAI configured successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error configuring Google GenAI (check API key validity and permissions): {e}\")\n",
    "        # sys.exit(1)\n",
    "\n",
    "# --- Hugging Face Hub Token ---\n",
    "print(\"\\nConfiguring Hugging Face Hub Token...\")\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if HF_TOKEN:\n",
    "    try:\n",
    "        hf_login(token=HF_TOKEN)\n",
    "        print(\"Successfully logged into Hugging Face Hub.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging into Hugging Face Hub: {e}\")\n",
    "        print(\"Some models from Hugging Face might require authentication to download.\")\n",
    "else:\n",
    "    print(\"HUGGINGFACE_HUB_TOKEN environment variable not found.\")\n",
    "    print(\"Proceeding without Hugging Face Hub login. This may be fine for public models.\")\n",
    "    print(\"For gated models or to ensure download rates, set the HUGGINGFACE_HUB_TOKEN environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d462401-ca76-477f-b7bb-6d3ed568c782",
   "metadata": {},
   "source": [
    "**2. CLIENT DEFINITIONS (SHARED ACROSS EXAMPLES)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab77f7-7676-4eae-b69e-21383bc8c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining client functions...\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.1. Local Text Embedding Client (SentenceTransformer)\n",
    "# ------------------------------------------------------------------------------\n",
    "_local_text_embedding_model = None\n",
    "_local_text_embedding_dim = None\n",
    "\n",
    "def get_local_text_embedding_client(model_name=\"all-MiniLM-L6-v2\"):\n",
    "    global _local_text_embedding_model, _local_text_embedding_dim\n",
    "    if _local_text_embedding_model is None:\n",
    "        try:\n",
    "            print(f\"Loading local text embedding model: {model_name}...\")\n",
    "            _local_text_embedding_model = SentenceTransformer(model_name)\n",
    "            _local_text_embedding_dim = _local_text_embedding_model.get_sentence_embedding_dimension()\n",
    "            print(f\"Local text embedding model '{model_name}' loaded. Dimension: {_local_text_embedding_dim}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading SentenceTransformer model '{model_name}': {e}\")\n",
    "            raise # Re-raise to stop if model loading fails\n",
    "\n",
    "    def embed_texts_local(texts: List[str]) -> np.ndarray:\n",
    "        if not _local_text_embedding_model:\n",
    "            raise RuntimeError(\"Local text embedding model not loaded.\")\n",
    "        if not texts:\n",
    "            return np.empty((0, _local_text_embedding_dim))\n",
    "        try:\n",
    "            embeddings = _local_text_embedding_model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error during local text embedding generation: {e}\")\n",
    "            return np.empty((0, _local_text_embedding_dim))\n",
    "    return embed_texts_local\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.2. Google Cloud Text Embedding Client\n",
    "# ------------------------------------------------------------------------------\n",
    "_google_embedding_model_name = \"models/embedding-001\"\n",
    "_google_embedding_dim = 768 # Dimension for models/embedding-001\n",
    "\n",
    "def embed_texts_google(texts: List[str]) -> np.ndarray:\n",
    "    if not GOOGLE_API_KEY or \"YOUR_API_KEY_HERE\" in GOOGLE_API_KEY:\n",
    "        print(\"Error: Google API Key not configured. Cannot use Google embedding client.\")\n",
    "        return np.empty((0, _google_embedding_dim))\n",
    "    if not texts:\n",
    "        return np.empty((0, _google_embedding_dim))\n",
    "    try:\n",
    "        result = genai.embed_content(\n",
    "            model=_google_embedding_model_name,\n",
    "            content=texts,\n",
    "            task_type=\"clustering\"\n",
    "        )\n",
    "        embeddings = np.array(result[\"embedding\"])\n",
    "        if embeddings.ndim != 2 or embeddings.shape[0] != len(texts) or embeddings.shape[1] != _google_embedding_dim:\n",
    "             print(f\"Warning: Unexpected Google embedding shape. Expected ({len(texts)}, {_google_embedding_dim}), Got {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Google Embedding API: {e}\")\n",
    "        return np.empty((0, _google_embedding_dim))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.3. Local LLM Client (Gemma)\n",
    "# ------------------------------------------------------------------------------\n",
    "_gemma_model = None\n",
    "_gemma_processor = None\n",
    "_gemma_model_id = \"google/gemma-3-4b-it\" # Using the 4B IT model from script\n",
    "\n",
    "def get_gemma_llm_client(model_id=_gemma_model_id):\n",
    "    global _gemma_model, _gemma_processor\n",
    "    if _gemma_model is None:\n",
    "        try:\n",
    "            print(f\"Loading Gemma model: {model_id}...\")\n",
    "            print(\"This may take a while and require significant RAM/VRAM.\")\n",
    "            # Determine data type based on GPU availability and capability\n",
    "            dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32\n",
    "            \n",
    "            _gemma_model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "                model_id, \n",
    "                device_map=\"auto\", # Automatically uses GPU if available\n",
    "                torch_dtype=dtype  # Use bfloat16 if supported, else float32\n",
    "            ).eval()\n",
    "            _gemma_processor = AutoProcessor.from_pretrained(model_id)\n",
    "            print(f\"Gemma model '{model_id}' loaded to device: {_gemma_model.device} with dtype: {dtype}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Gemma model '{model_id}': {e}\")\n",
    "            print(\"Ensure you have 'transformers', 'torch', and 'accelerate' installed.\")\n",
    "            print(\"A powerful GPU with sufficient VRAM is recommended for larger Gemma models.\")\n",
    "            raise\n",
    "\n",
    "    def gemma_llm_function(prompt: str) -> str:\n",
    "        if not _gemma_model or not _gemma_processor:\n",
    "            raise RuntimeError(\"Gemma model/processor not loaded.\")\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}\n",
    "        ]\n",
    "        try:\n",
    "            inputs = _gemma_processor.apply_chat_template(\n",
    "                messages, add_generation_prompt=True, tokenize=True,\n",
    "                return_dict=True, return_tensors=\"pt\"\n",
    "            ).to(_gemma_model.device) # Ensure inputs are on the same device as the model\n",
    "\n",
    "            input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                generation = _gemma_model.generate(**inputs, max_new_tokens=150, do_sample=False) # Increased max_new_tokens for better summaries\n",
    "                generation = generation[0][input_len:]\n",
    "            \n",
    "            decoded = _gemma_processor.decode(generation, skip_special_tokens=True)\n",
    "            return decoded.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Gemma LLM generation: {e}\")\n",
    "            return \"\"\n",
    "    return gemma_llm_function\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.4. Google Cloud LLM Client (Gemini)\n",
    "# ------------------------------------------------------------------------------\n",
    "_gemini_model_name = \"gemini-2.0-flash\"\n",
    "_gemini_model_genai = None\n",
    "\n",
    "def get_gemini_llm_client(model_name=_gemini_model_name):\n",
    "    global _gemini_model_genai\n",
    "    if not GOOGLE_API_KEY or \"YOUR_API_KEY_HERE\" in GOOGLE_API_KEY:\n",
    "        print(\"Error: Google API Key not configured. Cannot initialize Gemini client.\")\n",
    "        # Return a dummy function that indicates error\n",
    "        def error_gemini_fn(prompt: str) -> str:\n",
    "            print(\"Gemini LLM client not available due to API key issue.\")\n",
    "            return \"\"\n",
    "        return error_gemini_fn\n",
    "\n",
    "    if _gemini_model_genai is None:\n",
    "        try:\n",
    "            print(f\"Initializing Gemini model: {model_name}...\")\n",
    "            _gemini_model_genai = genai.GenerativeModel(model_name)\n",
    "            print(f\"Gemini model '{model_name}' initialized.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Gemini model '{model_name}': {e}\")\n",
    "            # Return a dummy function that indicates error\n",
    "            def error_gemini_fn_init(prompt: str) -> str:\n",
    "                print(f\"Gemini LLM client not available due to initialization error: {e}\")\n",
    "                return \"\"\n",
    "            return error_gemini_fn_init\n",
    "\n",
    "    def gemini_llm_function(prompt: str) -> str:\n",
    "        if not _gemini_model_genai:\n",
    "            # This case should ideally be caught by the API key check or init check\n",
    "            print(\"Gemini model not available (was not initialized).\")\n",
    "            return \"\"\n",
    "        try:\n",
    "            response = _gemini_model_genai.generate_content(prompt)\n",
    "            if not response.candidates:\n",
    "                print(\"Warning: Gemini response blocked or empty.\")\n",
    "                # You can inspect response.prompt_feedback or response.candidates[0].finish_reason for details\n",
    "                return \"\"\n",
    "            return response.text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling Gemini API: {e}\")\n",
    "            return \"\"\n",
    "    return gemini_llm_function\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.5. Local Image Embedding Client (CLIP)\n",
    "# ------------------------------------------------------------------------------\n",
    "DEFAULT_MAX_IMAGE_DIMENSION = 512 # Max width/height for images (pixels)\n",
    "print(f\"Default max image dimension set to: {DEFAULT_MAX_IMAGE_DIMENSION}px\")\n",
    "\n",
    "def resize_pil_image(image: Image.Image, max_dim: int = DEFAULT_MAX_IMAGE_DIMENSION) -> Image.Image:\n",
    "    \"\"\"Resizes a PIL Image to have its largest dimension be max_dim, preserving aspect ratio.\"\"\"\n",
    "    try:\n",
    "        # Use thumbnail as it resizes in-place and maintains aspect ratio\n",
    "        # It modifies the image to contain a thumbnail version of itself, no larger than the given size.\n",
    "        original_size = image.size\n",
    "        image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS) # LANCZOS is good for downscaling\n",
    "        # print(f\"Resized image from {original_size} to {image.size}\") # Optional: for debugging\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image resizing: {e}\")\n",
    "        return image # Return original image on error\n",
    "\n",
    "_clip_model = None\n",
    "def get_clip_image_embedding_client(model_name=\"clip-ViT-B-32\", max_dim: int = DEFAULT_MAX_IMAGE_DIMENSION): # Added max_dim\n",
    "    global _clip_model\n",
    "    if _clip_model is None:\n",
    "        try:\n",
    "            print(f\"Loading CLIP model: {model_name}...\")\n",
    "            _clip_model = SentenceTransformer(model_name)\n",
    "            print(f\"CLIP model '{model_name}' loaded.\")\n",
    "        except Exception as e: print(f\"Error loading CLIP model '{model_name}': {e}\"); raise\n",
    "\n",
    "    def embed_images_clip(image_identifiers: List[Any]) -> np.ndarray:\n",
    "        if not _clip_model: raise RuntimeError(\"CLIP model not loaded.\")\n",
    "        clip_dim = _clip_model.get_sentence_embedding_dimension()\n",
    "        if not image_identifiers: return np.empty((0, clip_dim))\n",
    "        \n",
    "        images_to_process = []\n",
    "        for identifier in image_identifiers:\n",
    "            try:\n",
    "                if isinstance(identifier, str): # Path\n",
    "                    img = Image.open(identifier).convert(\"RGB\")\n",
    "                    img = resize_pil_image(img, max_dim) # Resize if loaded from path\n",
    "                    images_to_process.append(img)\n",
    "                elif isinstance(identifier, Image.Image): # PIL Image\n",
    "                    # Assuming PIL images passed directly are already appropriately sized or resizing is handled upstream.\n",
    "                    # For consistency, we can also resize here, but be mindful if user intentionally passed a specific size.\n",
    "                    # img = resize_pil_image(identifier, max_dim) # Uncomment if all PIL images should be resized by client\n",
    "                    images_to_process.append(identifier.convert(\"RGB\"))\n",
    "                else: \n",
    "                    print(f\"Warning: Skipping unsupported image id type: {type(identifier)}\")\n",
    "                    continue\n",
    "            except Exception as e: \n",
    "                print(f\"Warning: Error loading/processing image '{identifier}': {e}. Skipping.\")\n",
    "        \n",
    "        if not images_to_process: return np.empty((0, clip_dim))\n",
    "        \n",
    "        try:\n",
    "            # print(f\"CLIP: Embedding {len(images_to_process)} images...\")\n",
    "            embeddings = _clip_model.encode(images_to_process, batch_size=32, convert_to_numpy=True, show_progress_bar=False)\n",
    "            # print(f\"CLIP: Embedding successful, shape {embeddings.shape}\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error during CLIP embedding: {e}\")\n",
    "            return np.empty((0, clip_dim))\n",
    "    return embed_images_clip\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.6. Local Image Captioning Client (BLIP)\n",
    "# ------------------------------------------------------------------------------\n",
    "_blip_processor = None\n",
    "_blip_model = None\n",
    "_blip_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_blip_image_captioning_client(model_name=\"Salesforce/blip-image-captioning-large\", max_dim: int = DEFAULT_MAX_IMAGE_DIMENSION): # Added max_dim\n",
    "    global _blip_processor, _blip_model\n",
    "    if _blip_processor is None or _blip_model is None:\n",
    "        try:\n",
    "            print(f\"Loading BLIP processor/model: {model_name} to device: {_blip_device}...\")\n",
    "            _blip_processor = BlipProcessor.from_pretrained(model_name)\n",
    "            _blip_model = BlipForConditionalGeneration.from_pretrained(model_name).to(_blip_device)\n",
    "            print(f\"BLIP model '{model_name}' loaded to {_blip_device}.\")\n",
    "        except Exception as e: print(f\"Error loading BLIP model/processor '{model_name}': {e}\"); raise\n",
    "\n",
    "    def caption_images_blip(image_identifiers: List[Any], prompt: Optional[str] = None) -> List[Optional[str]]:\n",
    "        if not _blip_processor or not _blip_model: raise RuntimeError(\"BLIP model/processor not loaded.\")\n",
    "        if not image_identifiers: return []\n",
    "        \n",
    "        images_to_process, valid_indices = [], []\n",
    "        for i, identifier in enumerate(image_identifiers):\n",
    "            try:\n",
    "                if isinstance(identifier, str): # Path\n",
    "                    img = Image.open(identifier).convert(\"RGB\")\n",
    "                    img = resize_pil_image(img, max_dim) # Resize if loaded from path\n",
    "                    images_to_process.append(img)\n",
    "                    valid_indices.append(i)\n",
    "                elif isinstance(identifier, Image.Image): # PIL Image\n",
    "                    # img = resize_pil_image(identifier, max_dim) # Uncomment if all PIL images should be resized by client\n",
    "                    images_to_process.append(identifier.convert(\"RGB\"))\n",
    "                    valid_indices.append(i)\n",
    "                else: \n",
    "                    print(f\"Warning: Skipping unsupported image id type: {type(identifier)}\")\n",
    "                    continue\n",
    "            except Exception as e: \n",
    "                print(f\"Warning: Error loading/processing image '{identifier}': {e}. Skipping.\")\n",
    "        \n",
    "        final_captions = [None] * len(image_identifiers)\n",
    "        if not images_to_process: return final_captions\n",
    "        \n",
    "        try:\n",
    "            # print(f\"BLIP: Captioning {len(images_to_process)} images...\")\n",
    "            if prompt: \n",
    "                inputs = _blip_processor(images=images_to_process, text=[prompt] * len(images_to_process), return_tensors=\"pt\", padding=True).to(_blip_device)\n",
    "            else: \n",
    "                inputs = _blip_processor(images=images_to_process, return_tensors=\"pt\", padding=True).to(_blip_device)\n",
    "            \n",
    "            with torch.no_grad(): \n",
    "                outputs = _blip_model.generate(**inputs, max_length=75, num_beams=3) # Increased max_length for potentially better captions\n",
    "            \n",
    "            generated_captions_raw = _blip_processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            for i_cap, cap_text in enumerate(generated_captions_raw): \n",
    "                final_captions[valid_indices[i_cap]] = cap_text.strip()\n",
    "            # print(f\"BLIP: Captioning successful.\")\n",
    "            return final_captions\n",
    "        except Exception as e:\n",
    "            print(f\"Error during BLIP captioning: {e}\")\n",
    "            return [None] * len(image_identifiers)\n",
    "    return caption_images_blip\n",
    "\n",
    "print(\"All client functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706ca3b-aa94-431d-acc5-8239cfe006e7",
   "metadata": {},
   "source": [
    "**3. HERCULES CLUSTERING EXAMPLES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78270c08-f7c1-410d-9ba9-0ebbeb5a6653",
   "metadata": {},
   "source": [
    "*Example 1: Local Text Clustering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142f821-0193-4d71-8b23-648319035f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- EXAMPLE 1: LOCAL TEXT CLUSTERING ---\")\n",
    "try:\n",
    "    # Instantiate clients for this example\n",
    "    print(\"Instantiating clients for Local Text Clustering...\")\n",
    "    local_text_embed_client = get_local_text_embedding_client()\n",
    "    # Note: Gemma model is large. Loading might take time and resources.\n",
    "    # If you encounter issues, you might not have enough RAM/VRAM.\n",
    "    try:\n",
    "        local_llm_client_gemma = get_gemma_llm_client()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load Gemma LLM client for local text clustering: {e}\")\n",
    "        print(\"Proceeding without LLM for this example, or using a fallback if defined.\")\n",
    "        local_llm_client_gemma = None # Or a fallback if you have one\n",
    "\n",
    "    # Sample Data\n",
    "    sample_texts_local = [\n",
    "        \"Introduction to machine learning concepts.\",\n",
    "        \"Advanced techniques in deep neural networks.\",\n",
    "        \"A guide to Python programming for beginners.\",\n",
    "        \"Web development using Flask and Jinja.\",\n",
    "        \"Understanding gradient descent and backpropagation.\",\n",
    "        \"Natural language processing with transformers.\",\n",
    "        \"Getting started with SQL databases.\",\n",
    "        \"Data structures and algorithms in Python.\"\n",
    "    ]\n",
    "    print(f\"Using {len(sample_texts_local)} sample text documents.\")\n",
    "\n",
    "    # Define Hierarchy\n",
    "    hierarchy_levels_local_text = [3, 2] # 3 top-level, then subdivide\n",
    "    print(f\"Target hierarchy levels: {hierarchy_levels_local_text}\")\n",
    "\n",
    "    # Instantiate Hercules\n",
    "    print(\"Initializing Hercules for local text clustering...\")\n",
    "    hercules_local_text = Hercules(\n",
    "        level_cluster_counts=hierarchy_levels_local_text,\n",
    "        representation_mode=\"direct\", # \"direct\" means use original item embeddings\n",
    "        text_embedding_client=local_text_embed_client,\n",
    "        llm_client=local_llm_client_gemma, # Using Gemma\n",
    "        image_embedding_client=None,\n",
    "        image_captioning_client=None,\n",
    "    )\n",
    "    print(\"Hercules initialized for local text clustering.\")\n",
    "\n",
    "    # Run Clustering\n",
    "    print(\"\\nStarting local text clustering process...\")\n",
    "    start_time_local_text = time.time()\n",
    "    if local_llm_client_gemma is None: # Check if LLM client failed to load\n",
    "        print(\"Warning: LLM client (Gemma) not available. Cluster names/summaries might be basic or missing.\")\n",
    "\n",
    "    top_clusters_local_text = hercules_local_text.cluster(sample_texts_local)\n",
    "    end_time_local_text = time.time()\n",
    "\n",
    "    if top_clusters_local_text:\n",
    "        print(f\"\\nFound {len(top_clusters_local_text)} top-level clusters for local text.\")\n",
    "        for i, cluster in enumerate(top_clusters_local_text):\n",
    "            print(f\"\\n--- Top Cluster {i+1} (ID: {cluster.id}) ---\")\n",
    "            cluster.print_hierarchy(indent_increment=hercules_local_text.cluster_print_indent_increment)\n",
    "    else:\n",
    "        print(\"No clusters were formed for local text.\")\n",
    "    print(f\"Local text clustering finished in {end_time_local_text - start_time_local_text:.2f} seconds.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during Local Text Clustering example: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45825c74-d44d-4038-b9a9-552c3a34d71f",
   "metadata": {},
   "source": [
    "*Example 2: Google Cloud Text Clustering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7ba93-9b13-40b4-88e2-810e52bc9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- EXAMPLE 2: GOOGLE CLOUD TEXT CLUSTERING ---\")\n",
    "if not GOOGLE_API_KEY or \"YOUR_API_KEY_HERE\" in GOOGLE_API_KEY:\n",
    "    print(\"Skipping Google Cloud Text Clustering example as GOOGLE_API_KEY is not set.\")\n",
    "else:\n",
    "    try:\n",
    "        # Instantiate clients for this example\n",
    "        print(\"Instantiating clients for Google Cloud Text Clustering...\")\n",
    "        google_text_embed_client = embed_texts_google # Direct function, not factory\n",
    "        google_llm_client_gemini = get_gemini_llm_client()\n",
    "\n",
    "        # Sample Data (can reuse from local example or define new)\n",
    "        sample_texts_google = [\n",
    "            \"The history of ancient Rome and its emperors.\",\n",
    "            \"Exploring the cosmos: galaxies, stars, and black holes.\",\n",
    "            \"Sustainable farming practices for a healthier planet.\",\n",
    "            \"The impact of social media on modern society.\",\n",
    "            \"Principles of quantum physics and string theory.\",\n",
    "            \"Renewable energy sources: solar, wind, and hydro.\",\n",
    "            \"Culinary arts: a journey through world cuisines.\",\n",
    "            \"The rise of artificial intelligence and its ethical implications.\"\n",
    "        ]\n",
    "        print(f\"Using {len(sample_texts_google)} sample text documents for Google Cloud.\")\n",
    "\n",
    "        # Define Hierarchy\n",
    "        hierarchy_levels_google_text = [3, 2]\n",
    "        print(f\"Target hierarchy levels: {hierarchy_levels_google_text}\")\n",
    "\n",
    "        # Instantiate Hercules\n",
    "        print(\"Initializing Hercules for Google Cloud text clustering...\")\n",
    "        hercules_google_text = Hercules(\n",
    "            level_cluster_counts=hierarchy_levels_google_text,\n",
    "            representation_mode=\"direct\",\n",
    "            text_embedding_client=google_text_embed_client,\n",
    "            llm_client=google_llm_client_gemini, # Using Gemini\n",
    "            image_embedding_client=None,\n",
    "            image_captioning_client=None,\n",
    "        )\n",
    "        print(\"Hercules initialized for Google Cloud text clustering.\")\n",
    "\n",
    "        # Run Clustering\n",
    "        print(\"\\nStarting Google Cloud text clustering process...\")\n",
    "        start_time_google_text = time.time()\n",
    "        top_clusters_google_text = hercules_google_text.cluster(sample_texts_google)\n",
    "        end_time_google_text = time.time()\n",
    "\n",
    "        if top_clusters_google_text:\n",
    "            print(f\"\\nFound {len(top_clusters_google_text)} top-level clusters for Google Cloud text.\")\n",
    "            for i, cluster in enumerate(top_clusters_google_text):\n",
    "                print(f\"\\n--- Top Cluster {i+1} (ID: {cluster.id}) ---\")\n",
    "                cluster.print_hierarchy(indent_increment=hercules_google_text.cluster_print_indent_increment)\n",
    "        else:\n",
    "            print(\"No clusters were formed for Google Cloud text.\")\n",
    "        print(f\"Google Cloud text clustering finished in {end_time_google_text - start_time_google_text:.2f} seconds.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during Google Cloud Text Clustering example: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a6e5e8-3fce-4d50-aaf6-547759156629",
   "metadata": {},
   "source": [
    "*Example 3: Local Image Clustering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a024c2e-db09-4793-a6f4-ad2d8ce89ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- EXAMPLE 3: LOCAL IMAGE CLUSTERING (PEXELS URLS) ---\")\n",
    "def load_image_from_url(url: str, timeout: int = 20) -> Optional[Image.Image]:\n",
    "    \"\"\"Downloads an image from a URL and returns it as a PIL.Image object.\"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading image from: {url}\")\n",
    "        response = requests.get(url, timeout=timeout)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors (4xx or 5xx)\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        print(f\"Successfully loaded image from: {url}\")\n",
    "        return img\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout error downloading image from {url}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error downloading image from {url}: {e}\")\n",
    "        return None\n",
    "    except IOError as e:\n",
    "        print(f\"Error opening image data from {url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred with {url}: {e}\")\n",
    "        return None\n",
    "try:\n",
    "    # Instantiate clients for this example\n",
    "    print(\"Instantiating clients for Local Image Clustering...\")\n",
    "    # Image specific clients\n",
    "    clip_image_embed_client = get_clip_image_embedding_client()\n",
    "    blip_image_caption_client = get_blip_image_captioning_client()\n",
    "    \n",
    "    local_text_embed_for_image_example = get_local_text_embedding_client()\n",
    "    \n",
    "    if not GOOGLE_API_KEY or \"YOUR_API_KEY_HERE\" in GOOGLE_API_KEY:\n",
    "        print(\"Warning: GOOGLE_API_KEY not set. LLM for image cluster naming will use a fallback (None) or fail if Gemini is strictly needed.\")\n",
    "        llm_for_image_example = None # Fallback: no LLM for naming\n",
    "    else:\n",
    "        llm_for_image_example = get_gemini_llm_client()\n",
    "\n",
    "\n",
    "    # Sample Data: Pexels URLs\n",
    "    # Using diverse images to encourage meaningful clusters\n",
    "    pexels_image_urls = [\n",
    "        \"https://images.pexels.com/photos/45201/kitty-cat-kitten-pet-45201.jpeg\",      # Cat 1\n",
    "        \"https://images.pexels.com/photos/1643457/pexels-photo-1643457.jpeg\",     # Cat 2\n",
    "        \"https://images.pexels.com/photos/1108099/pexels-photo-1108099.jpeg\",     # Dogs\n",
    "        \"https://images.pexels.com/photos/170811/pexels-photo-170811.jpeg\",       # Car\n",
    "        \"https://images.pexels.com/photos/3763313/pexels-photo-3763313.jpeg\",     # Dog\n",
    "        \"https://images.pexels.com/photos/346529/pexels-photo-346529.jpeg\",     # Landscape (Mountains, Lake)\n",
    "        \"https://images.pexels.com/photos/1640777/pexels-photo-1640777.jpeg\",     # Food (Salad Bowl)\n",
    "        \"https://images.pexels.com/photos/208701/pexels-photo-208701.jpeg\",       # Architecture (Building)\n",
    "        \"https://images.pexels.com/photos/994605/pexels-photo-994605.jpeg\"        # Landscape (Beach sunset)\n",
    "    ]\n",
    "    print(f\"Defined {len(pexels_image_urls)} Pexels image URLs.\")\n",
    "\n",
    "    # Download and load images\n",
    "    print(\"Downloading and loading images...\")\n",
    "    sample_pil_images = []\n",
    "    for url in pexels_image_urls:\n",
    "        img = load_image_from_url(url)\n",
    "        if img:\n",
    "            sample_pil_images.append(img)\n",
    "\n",
    "    if not sample_pil_images:\n",
    "        print(\"Error: No images were successfully loaded. Skipping image clustering example.\")\n",
    "    else:\n",
    "        print(f\"Successfully loaded {len(sample_pil_images)} images for clustering.\")\n",
    "\n",
    "        # Define Hierarchy\n",
    "        hierarchy_levels_image = [3, 2] # Adjust as needed based on number of images\n",
    "        print(f\"Target hierarchy levels for images: {hierarchy_levels_image}\")\n",
    "\n",
    "        # Instantiate Hercules\n",
    "        print(\"Initializing Hercules for local image clustering...\")\n",
    "        hercules_image = Hercules(\n",
    "            level_cluster_counts=hierarchy_levels_image,\n",
    "            representation_mode=\"direct\", # Use direct image embeddings\n",
    "            image_embedding_client=clip_image_embed_client,\n",
    "            image_captioning_client=blip_image_caption_client, # Used if mode was 'caption' or for some internal processing\n",
    "            llm_client=llm_for_image_example, # For cluster naming/summarization\n",
    "            text_embedding_client=local_text_embed_for_image_example, # For processing any text (e.g. captions if used)\n",
    "        )\n",
    "        print(\"Hercules initialized for local image clustering.\")\n",
    "\n",
    "        # Run Clustering\n",
    "        print(\"\\nStarting image clustering process...\")\n",
    "        start_time_image = time.time()\n",
    "        if llm_for_image_example is None:\n",
    "            print(\"Warning: LLM client (Gemini) not available for image example. Cluster names/summaries might be basic or missing.\")\n",
    "\n",
    "        top_clusters_image = hercules_image.cluster(sample_pil_images)\n",
    "        end_time_image = time.time()\n",
    "\n",
    "        if top_clusters_image:\n",
    "            print(f\"\\nFound {len(top_clusters_image)} top-level clusters for images.\")\n",
    "            for i, cluster in enumerate(top_clusters_image):\n",
    "                print(f\"\\n--- Top Cluster {i+1} (ID: {cluster.id}) ---\")\n",
    "                cluster.print_hierarchy(indent_increment=hercules_image.cluster_print_indent_increment)\n",
    "        else:\n",
    "            print(\"No clusters were formed for images.\")\n",
    "        print(f\"Image clustering finished in {end_time_image - start_time_image:.2f} seconds.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during Local Image Clustering example: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7968f20-1178-4d5c-9d52-d6c3b1be1b0c",
   "metadata": {},
   "source": [
    "*Example 4: Numeric Data Clustering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c4774-9855-4657-a838-47210d04ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- EXAMPLE 4: DATAFRAME (NUMERIC) CLUSTERING ---\")\n",
    "try:\n",
    "    print(\"Instantiating clients for DataFrame Clustering...\")\n",
    "    local_text_embed_client_ex4 = get_local_text_embedding_client()\n",
    "    \n",
    "    llm_client_for_df_ex4 = None\n",
    "    print(\"Attempting to use Gemini for DataFrame cluster naming (if API key available), else Gemma, else None.\")\n",
    "    if GOOGLE_API_KEY and \"YOUR_API_KEY_HERE\" not in GOOGLE_API_KEY:\n",
    "        llm_client_for_df_ex4 = get_gemini_llm_client()\n",
    "    \n",
    "    if llm_client_for_df_ex4 is None: # If Gemini not used or failed\n",
    "        try:\n",
    "            llm_client_for_df_ex4 = get_gemma_llm_client()\n",
    "            print(\"Using Gemma for DataFrame clustering LLM.\")\n",
    "        except Exception as e_gemma_df:\n",
    "            print(f\"Could not load Gemma for DataFrame clustering: {e_gemma_df}. Proceeding without LLM.\")\n",
    "            llm_client_for_df_ex4 = None\n",
    "\n",
    "\n",
    "    # Sample DataFrame: Customer Segments\n",
    "    data = {\n",
    "        'CustomerID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "        'Age': [25, 65, 30, 70, 22, 58, 45, 48, 28, 68, 35, 62],\n",
    "        'AnnualIncome_k': [40, 120, 50, 110, 35, 90, 70, 75, 45, 130, 60, 100],\n",
    "        'SpendingScore_1_100': [80, 20, 75, 15, 85, 30, 55, 50, 82, 22, 70, 25],\n",
    "        'YearsAsCustomer': [1, 10, 2, 12, 1, 8, 5, 6, 2, 11, 3, 9]\n",
    "    }\n",
    "    sample_df = pd.DataFrame(data)\n",
    "    \n",
    "    features_for_clustering_df = sample_df[['Age', 'AnnualIncome_k', 'SpendingScore_1_100', 'YearsAsCustomer']]\n",
    "    \n",
    "    print(f\"Using a DataFrame with {features_for_clustering_df.shape[0]} samples and {features_for_clustering_df.shape[1]} features for clustering:\")\n",
    "    print(features_for_clustering_df.head())\n",
    "\n",
    "    # Define Hierarchy\n",
    "    hierarchy_levels_df = [3, 2] # Target: 3 top-level clusters, then attempt to subdivide\n",
    "    print(f\"Target hierarchy levels: {hierarchy_levels_df}\")\n",
    "\n",
    "    print(\"Initializing Hercules for DataFrame clustering...\")\n",
    "    hercules_df = Hercules(\n",
    "        level_cluster_counts=hierarchy_levels_df,\n",
    "        representation_mode=\"direct\", # Use the numeric feature vectors directly\n",
    "        text_embedding_client=local_text_embed_client_ex4,\n",
    "        llm_client=llm_client_for_df_ex4,\n",
    "        image_embedding_client=None,\n",
    "        image_captioning_client=None,\n",
    "    )\n",
    "    print(\"Hercules initialized for DataFrame clustering.\")\n",
    "\n",
    "    print(\"\\nStarting DataFrame clustering process...\")\n",
    "    if llm_client_for_df_ex4 is None: print(\"Warning: LLM client not available for DataFrame example. Cluster names might be basic.\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    top_clusters_df = hercules_df.cluster(features_for_clustering_df)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if top_clusters_df:\n",
    "        print(f\"\\nFound {len(top_clusters_df)} top-level clusters for DataFrame data.\")\n",
    "        for i, cluster in enumerate(top_clusters_df):\n",
    "            print(f\"\\n--- Top Cluster {i+1} (ID: {cluster.id}) ---\")\n",
    "            cluster.print_hierarchy(indent_increment=hercules_df.cluster_print_indent_increment)            \n",
    "    else:\n",
    "        print(\"No clusters were formed for DataFrame data.\")\n",
    "    print(f\"DataFrame clustering finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during DataFrame Clustering example: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
